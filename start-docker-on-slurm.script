#!/bin/bash
#SBATCH --mail-type=ALL
#SBATCH --mail-user=ejansen@nvidia.com
#SBATCH --output=outfile-%J
#SBATCH --partition=A6000
#SBATCH --time=04:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH packjob
#SBATCH --partition=3090
#SBATCH --time=04:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1

## Show ouput
set -eux

## JAR names and download URL's (update when required)
CUDF_URL="https://storage.googleapis.com/mirror_rapids_ej/cudf-0.18-20210112.093909-33-cuda11.jar"
CUDF_NAME="cudf-0.18-20210112.093909-33-cuda11.jar"
RAPIDS_SPARK_URL="https://storage.googleapis.com/mirror_rapids_ej/rapids-4-spark_2.12-0.4.0-20210112.085853-45.jar"
RAPIDS_SPARK_NAME="rapids-4-spark_2.12-0.4.0-20210112.085853-45.jar"
RAPIDS_SPARK_SQL_URL="https://storage.googleapis.com/mirror_rapids_ej/rapids-4-spark-sql_2.12-0.4.0-20210112.085254-46.jar"
RAPIDS_SPARK_SQL_NAME="rapids-4-spark-sql_2.12-0.4.0-20210112.085254-46.jar"
RAPIDS_TESTS_URL="https://storage.googleapis.com/mirror_rapids_ej/rapids-4-spark-integration-tests_2.12-0.4.0-20210112.090812-45-jar-with-dependencies.jar"
RAPIDS_TESTS_NAME="rapids-4-spark-integration-tests_2.12-0.4.0-20210112.090812-45-jar-with-dependencies.jar"

## Set root mountpoint
export MOUNT="/mnt/nvdl/datasets/tpcds"

## Mountpoint for config (shared filesystem required)
export CONFIG_MOUNT="${MOUNT}/configuration"

## Mountpoint for dataset (shared filesystem required)
export DATA_MOUNT="${MOUNT}/dataset"

## true/false run the benchmark 
## When true the benchmark will run and once finsihed the cluster will be stopped, results collected and SLURM job stopped
## When false slurm will sleep and the spark cluster is online and running
RUN_BENCHMARK="true"

## true/false dataset generation
## When true a dataset will be generated with the size of scalefactor * 1G and converted to parquet
DATAGEN="false"
SCALEFACTOR=100

## TPCDS file settings
INPUT_PATH="file://${DATA_MOUNT}/parquet"
OUTPUT_PATH="file://${DATA_MOUNT}/output"

## TPCDS s3 settings
##INPUT_PATH="gs://ec-benchmark-data/tpc-ds/tpcds_sf3000-parquet/useDecimal=false,useDate=true,filterNull=false"
##OUTPUT_PATH="file://${MOUNT}/output"
S3_ENDPOINT="https://storage.googleapis.com"
S3A_CREDS_USR=""
S3A_CREDS_PSW=""

## TPCDS format settings
## Work in progress does nothing
INPUT_FORMAT="parquet"
OUTPUT_FORMAT="parquet"

## TPCDS test settings
BENCHMARK="tpcds"
ITERATIONS=1
##QUERY="q1 q2"
##QUERY="q1 q2 q3 q4 q5 q6 q7 q8 q9 q10"
QUERY="q1 q2 q3 q4 q5 q6 q7 q8 q9 q10 \
 q11 q12 q13 q15 q17 q18 q19 \
 q20 q21 q22 q25 q26 q27 q28 q29 \
 q30 q31 q32 q33 q34 q35 q36 q37 q38 q39a q39b \
 q40 q41 q42 q43 q44 q45 q46 q47 q48 q49 \
 q50 q51 q52 q53 q54 q55 q56 q57 q58 q59 \
 q60 q61 q62 q63 q64 q65 q66 q68 q69 \
 q70 q71 q73 q74 q75 q76 q78 q79 \
 q80 q81 q82 q83 q84 q85 q86 q87 q88 q89 \
 q90 q91 q92 q93 q94 q95 q96 q97 q98 q99"

## Enable or disable GPU with "true" or "false"
ENABLE_GPU="true"

## Set threads per GPU (1)
CONCURRENTGPU=2

## Set shuffle partitions
## $((${NUM_EXECUTORS}*${NUM_EXECUTOR_CORES}*2))
##SHUFFLE_PARTITIONS=128

## Set Spark SQL partition size ("128M")
MAXPARTITIONBYTES="256M"

## Configure driver memory ("10240M")
DRIVER_MEMORY="10240M"

## Set SPILL to storage size ("16384M")
SPILL_STORAGE_SIZE="4096M"

##*** No manual input of variables beyond this point ***##
##########################################################

## Set Spark master
MASTER=`hostname`

## Set TPCDS home
export TPCDS_HOME="${CONFIG_MOUNT}/tpcds-kit/tools"

## Map ID
IDU=$(id -u)
IDG=$(id -g)

mkdir -p ${CONFIG_MOUNT}/job/${SLURM_JOB_ID}
mkdir -p ${CONFIG_MOUNT}/job/${SLURM_JOB_ID}/conf
mkdir -p ${CONFIG_MOUNT}/job/${SLURM_JOB_ID}/results
mkdir -p ${CONFIG_MOUNT}/job/${SLURM_JOB_ID}/history

## Check & remove stale job config directories
echo Checking for stale job directories

JOBS=$(ls -x ${CONFIG_MOUNT}/job)
QUEUE=$(squeue |awk '(NR>1) {print $1}'|sed 's/[+].*$//' |sort -u)

for j in ${QUEUE}; do
JOBS=("${JOBS/$j}" )
done

echo deleting the following directories: ${JOBS}
for j in ${JOBS}; do
        rm -rf ${CONFIG_MOUNT}/job/${j}
done

if [ ! -f "${CONFIG_MOUNT}/tpcds-kit/tools/dsdgen" ]
then
    cp -r tpcds-kit ${CONFIG_MOUNT}/
    chmod +x ${CONFIG_MOUNT}/tpcds-kit/tools/dsdgen
else
    echo "tpcds-kit exists"
fi

if [ ! -f "${CONFIG_MOUNT}/sparkRapidsPlugin/getGpusResources.sh" ]
then
    cp -r sparkRapidsPlugin ${CONFIG_MOUNT}/
else
    echo "getGpusResources.sh exists"
fi

## Set RAPIDS dir name
SPARK_RAPIDS_DIR="${CONFIG_MOUNT}/sparkRapidsPlugin"

## Set SPARK CUDF JAR
if [ ! -f "${SPARK_RAPIDS_DIR}/${CUDF_NAME}" ]
then
    wget -P ${SPARK_RAPIDS_DIR} -c ${CUDF_URL}
else
    echo "${CUDF_NAME} exists"
fi
SPARK_CUDF_JAR="${SPARK_RAPIDS_DIR}/${CUDF_NAME}"

## Set RAPIDS 4 SPARK JAR
if [ ! -f "${SPARK_RAPIDS_DIR}/${RAPIDS_SPARK_NAME}" ]
then
    wget -P ${SPARK_RAPIDS_DIR} -c ${RAPIDS_SPARK_URL}
else
    echo "${RAPIDS_SPARK_NAME} exists"
fi
SPARK_RAPIDS_PLUGIN_JAR="${SPARK_RAPIDS_DIR}/${RAPIDS_SPARK_NAME}"

## Set RAPIDS test JAR
if [ ! -f "${SPARK_RAPIDS_DIR}/${RAPIDS_TESTS_NAME}" ]
then
    wget -P ${SPARK_RAPIDS_DIR} -c ${RAPIDS_TESTS_URL}
else
    echo "${RAPIDS_TESTS_NAME} exists"
fi
SPARK_RAPIDS_PLUGIN_INTEGRATION_TEST_JAR="${SPARK_RAPIDS_DIR}/${RAPIDS_TESTS_NAME}"

## Set SPARK SQL JAR
if [ ! -f "${SPARK_RAPIDS_DIR}/${RAPIDS_SPARK_SQL_NAME}" ]
then
    wget -P ${SPARK_RAPIDS_DIR} -c ${RAPIDS_SPARK_SQL_URL}
else
    echo "${RAPIDS_SPARK_SQL_NAME} exists"
fi
SPARK_SQL_JAR="${SPARK_RAPIDS_DIR}/${RAPIDS_SPARK_SQL_NAME}"

## Get number of CPU's per node
SLURM_CPUS_ON_NODE_PACK_GROUP_1=$(srun --pack-group=1 -n 1 -N 1 nproc)

## Calculate total cluster cores
TOTAL_CORES=$(( ${SLURM_CPUS_ON_NODE_PACK_GROUP_1} * ${SLURM_JOB_NUM_NODES_PACK_GROUP_1} ))
SPARK_WORKER_CORES=${SLURM_CPUS_ON_NODE_PACK_GROUP_1}

## Get amount of memory per node
SPARK_WORKER_MEMORY=$(( $(srun --pack-group=1 -n 1 -N 1 slurmd -C |awk 'match($0, /RealMemory=/)  { print substr($0, RSTART)}'|sed -e s/RealMemory=//g)*9/10))

if [ ${ENABLE_GPU} = "false" ]
  then 
       GPU_PER_NODE=0
       DOCKER_GPUS="none"
       NUM_EXECUTORS=${SLURM_JOB_NUM_NODES_PACK_GROUP_1}
       EXECUTOR_MEMORY=${SPARK_WORKER_MEMORY}
       RESOURCE_GPU_AMT=0
       WORKER_OPTS=""
       CONCURRENTGPU=1
  else 
       GPU_PER_NODE=$(srun --pack-group=1 -n 1 -N 1 nvidia-smi -L |grep -c GPU)
##       GPU_PER_NODE=2
       DOCKER_GPUS="all"
       NUM_EXECUTORS=$(( ${GPU_PER_NODE} * ${SLURM_JOB_NUM_NODES_PACK_GROUP_1} ))
       EXECUTOR_MEMORY=$(( ${SPARK_WORKER_MEMORY} / ${GPU_PER_NODE} ))
       PINNED_POOL_SIZE=$(( ${EXECUTOR_MEMORY} / 8 / ${CONCURRENTGPU} ))M
       if (( ${EXECUTOR_MEMORY} > 30720 )); then
       EXECUTOR_MEMORY=30720
       fi
       RESOURCE_GPU_AMT=$( awk -v e="${NUM_EXECUTORS}" -v c="${TOTAL_CORES}" 'BEGIN { printf "%s", e/c }' </dev/null )
       WORKER_OPTS="-Dspark.worker.resource.gpu.amount=${GPU_PER_NODE} -Dspark.worker.resource.gpu.discoveryScript=${CONFIG_MOUNT}/sparkRapidsPlugin/getGpusResources.sh"
fi

NUM_EXECUTOR_CORES=$(( ${TOTAL_CORES} / ${NUM_EXECUTORS} ))
SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}M
EXECUTOR_MEMORY=${EXECUTOR_MEMORY}M
SHUFFLE_PARTITIONS=$((${NUM_EXECUTORS}*${NUM_EXECUTOR_CORES}*2))

## Kill old instances when needed
srun --pack-group=0 -n 1 -N 1 -w `hostname` bash -c 'if [ $(docker ps |grep -c master) -eq 1 ] ; then docker kill master && echo "killing master"; fi'
srun --pack-group=1 --ntasks="${SLURM_JOB_NUM_NODES_PACK_GROUP_1}" bash -c 'if [ $(docker ps |grep -c worker) -eq 1 ] ; then docker kill worker && echo "killing worker"; fi'

## Dump worker nodes to slaves file
scontrol show hostname $SLURM_JOB_NODELIST_PACK_GROUP_1 > ${CONFIG_MOUNT}/job/${SLURM_JOB_ID}/conf/slaves

## Setup default spark configuration
conf=${CONFIG_MOUNT}/job/${SLURM_JOB_ID}/conf/spark-defaults.conf
echo "spark.default.parallelism" $((${NUM_EXECUTORS}*${NUM_EXECUTOR_CORES}*2)) > $conf
echo "spark.submit.deployMode" client >> $conf
echo "spark.master" spark://`hostname`:7077 >> $conf
echo "spark.executor.cores" ${NUM_EXECUTOR_CORES} >> $conf
echo "spark.executor.memory" ${EXECUTOR_MEMORY} >> $conf
echo "spark.eventLog.enabled" true >> $conf
echo "spark.eventLog.dir" file:${CONFIG_MOUNT}/job/${SLURM_JOB_ID}/history/ >> $conf
echo "spark.history.fs.logDirectory" file:${CONFIG_MOUNT}/job/${SLURM_JOB_ID}/history/ >> $conf

## Enable when existing image needs to be deleted
##srun --pack-group=0 -n 1 -N 1 -w `hostname` docker rmi gcr.io/data-science-enterprise/spark-master-slurm:3.0.1 || true

srun --pack-group=0 -n 1 -N 1 -w `hostname` docker run -dit \
-e MASTER="${MASTER}" \
-e ENABLE_GPU="${ENABLE_GPU}" \
-e SPARK_WORKER_CORES=${SPARK_WORKER_CORES} \
-e SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY} \
-e SPARK_WORKER_OPTS="${WORKER_OPTS}" \
-e CONCURRENTGPU="${CONCURRENTGPU}" \
-e TOTAL_CORES="${TOTAL_CORES}" \
-e NUM_EXECUTORS="${NUM_EXECUTORS}" \
-e NUM_EXECUTOR_CORES="${NUM_EXECUTOR_CORES}" \
-e EXECUTOR_MEMORY="${EXECUTOR_MEMORY}" \
-e PINNED_POOL_SIZE="${PINNED_POOL_SIZE}" \
-e DRIVER_MEMORY="${DRIVER_MEMORY}" \
-e SHUFFLE_PARTITIONS="${SHUFFLE_PARTITIONS}" \
-e MAXPARTITIONBYTES="${MAXPARTITIONBYTES}" \
-e SPILL_STORAGE_SIZE="${SPILL_STORAGE_SIZE}" \
-e S3A_CREDS_USR="${S3A_CREDS_USR}" \
-e S3A_CREDS_PSW="${S3A_CREDS_PSW}" \
-e S3_ENDPOINT="${S3_ENDPOINT}" \
-e OUTPUT_PATH="${OUTPUT_PATH}" \
-e INPUT_PATH="${INPUT_PATH}" \
-e INPUT_FORMAT="${INPUT_FORMAT}" \
-e OUTPUT_FORMAT="${OUTPUT_FORMAT}" \
-e SPARK_RAPIDS_PLUGIN_JAR="${SPARK_RAPIDS_PLUGIN_JAR}" \
-e SPARK_RAPIDS_PLUGIN_INTEGRATION_TEST_JAR="${SPARK_RAPIDS_PLUGIN_INTEGRATION_TEST_JAR}" \
-e SPARK_CUDF_JAR="${SPARK_CUDF_JAR}" \
-e SPARK_SQL_JAR="${SPARK_SQL_JAR}" \
-e BENCHMARK="${BENCHMARK}" \
-e ITERATIONS="${ITERATIONS}" \
-e SLURM_JOB_ID="${SLURM_JOB_ID}" \
-e QUERY="${QUERY}" \
-e SCALEFACTOR="${SCALEFACTOR}" \
-e TPCDS_HOME="${TPCDS_HOME}" \
-e RESOURCE_GPU_AMT="${RESOURCE_GPU_AMT}" \
-e IDU="${IDU}" \
-e IDG="${IDG}" \
-e MOUNT="${MOUNT}" \
-e CONFIG_MOUNT="${CONFIG_MOUNT}" \
-e DATA_MOUNT="${DATA_MOUNT}" \
-v ${CONFIG_MOUNT}/job/${SLURM_JOB_ID}/conf:/opt/spark/conf \
-v ${MOUNT}:${MOUNT} \
-v /tmp:/tmp \
--network host \
--name master \
--rm \
gcr.io/data-science-enterprise/spark-master-slurm:3.0.1

## Enable when existing image needs to be deleted
##srun --pack-group=1 --ntasks=${SLURM_JOB_NUM_NODES_HET_GROUP_1} --ntasks-per-node=1 docker rmi gcr.io/data-science-enterprise/spark-worker-slurm:3.0.1 || true

srun --pack-group=1 --ntasks=${SLURM_JOB_NUM_NODES_PACK_GROUP_1} --ntasks-per-node=1 docker run -dit \
-e MASTER=${MASTER} \
-e SPARK_WORKER_CORES=${SPARK_WORKER_CORES} \
-e SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY} \
-e NUM_EXECUTOR_CORES="${NUM_EXECUTOR_CORES}" \
-e SPARK_WORKER_OPTS="${WORKER_OPTS}" \
-e SPARK_RAPIDS_PLUGIN_JAR="${SPARK_RAPIDS_PLUGIN_JAR}" \
-e SPARK_RAPIDS_PLUGIN_INTEGRATION_TEST_JAR="${SPARK_RAPIDS_PLUGIN_INTEGRATION_TEST_JAR}" \
-e SPARK_CUDF_JAR="${SPARK_CUDF_JAR}" \
-e TPCDS_HOME="${TPCDS_HOME}" \
-v ${CONFIG_MOUNT}/job/${SLURM_JOB_ID}/conf:/opt/spark/conf \
-v ${MOUNT}:${MOUNT} \
-v /tmp:/tmp \
--gpus ${DOCKER_GPUS} \
--network host \
--name worker \
--rm \
gcr.io/data-science-enterprise/spark-worker-slurm:3.0.1

## Wait for workers to be registered
STEPTIME=3
NUM_WORKERS=$(cat ${CONFIG_MOUNT}/job/${SLURM_JOB_ID}/conf/slaves|wc -l)

echo number of workers to be registered: ${NUM_WORKERS}
for i in {1..100}
do
  sleep ${STEPTIME}
  NUM_REG=$(srun --pack-group=0 -n 1 -N 1 -w `hostname` docker logs master |grep -c "Registering worker")
  if [ ${NUM_REG} -eq ${NUM_WORKERS} ]
  then
     break
  fi
done
echo registered workers after $((i * ${STEPTIME})) seconds: ${NUM_REG}
echo "All workers registered!"

## Run data generation when enabled
if [ "${DATAGEN}" == "true" ];
then
      mkdir -p ${DATA_MOUNT}/csv
      mkdir -p ${DATA_MOUNT}/parquet
      srun --pack-group=0 -n 1 -N 1 -w `hostname` docker exec -i master /bin/sh -c 'rm -rf ${DATA_MOUNT}/csv/* ${DATA_MOUNT}/parquet/*'
      INPUT_PATH="file://${DATA_MOUNT}/parquet"
      srun --pack-group=0 -n 1 -N 1 -w `hostname` docker exec -i master /bin/sh -c 'cd ${TPCDS_HOME}; ./dsdgen -DIR ${DATA_MOUNT}/csv -SCALE ${SCALEFACTOR} -VERBOSE Y -FORCE Y'
      srun --pack-group=0 -n 1 -N 1 -w `hostname` docker exec -i master /bin/sh -c '${SPARK_HOME}/bin/spark-submit \
    --master spark://${MASTER}:7077 \
    --jars ${SPARK_RAPIDS_PLUGIN_JAR},${SPARK_CUDF_JAR} \
    --class com.nvidia.spark.rapids.tests.tpcds.ConvertFiles \
    ${SPARK_RAPIDS_PLUGIN_INTEGRATION_TEST_JAR} \
    --input ${DATA_MOUNT}/csv \
    --output ${DATA_MOUNT}/parquet \
    --output-format parquet \
    --coalesce customer_address=1 \
    --repartition web_sales=${SHUFFLE_PARTITIONS} inventory=${SHUFFLE_PARTITIONS}'
fi

## Run TPCDS like benchmark when enabled if not sleep
if [ "${RUN_BENCHMARK}" == "true" ];
then
    srun --pack-group=0 -n 1 -N 1 -w `hostname` docker exec -i master /bin/sh -c 'cd ${CONFIG_MOUNT}/job/${SLURM_JOB_ID}/results; for i in ${QUERY}; do ${SPARK_HOME}/bin/spark-submit \
    --master spark://${MASTER}:7077 \
    --conf spark.driver.memory=${DRIVER_MEMORY} \
    --conf spark.rapids.memory.host.spillStorageSize=${SPILL_STORAGE_SIZE} \
    --conf spark.sql.adaptive.enabled=false \
    --conf spark.sql.files.maxPartitionBytes=${MAXPARTITIONBYTES} \
    --conf spark.sql.shuffle.partitions=${SHUFFLE_PARTITIONS} \
    --conf spark.sql.autoBroadcastJoinThreshold=1024MB \
    --conf spark.shuffle.consolidateFiles=true \
    --conf spark.executor.resource.gpu.amount=1 \
    --conf spark.executor.heartbeatInterval=300s \
    --conf spark.task.resource.gpu.amount=${RESOURCE_GPU_AMT} \
    --conf spark.rapids.memory.pinnedPool.size=${PINNED_POOL_SIZE} \
    --conf spark.rapids.memory.gpu.pool=DEFAULT \
    --conf spark.rapids.memory.gpu.allocFraction=0.9 \
    --conf spark.rapids.shuffle.transport.enabled=true \
    --conf spark.rapids.sql.batchSizeRows=2147483647 \
    --conf spark.rapids.sql.explain=ALL \
    --conf spark.rapids.sql.concurrentGpuTasks=${CONCURRENTGPU} \
    --conf spark.rapids.sql.format.parquet.enabled=true \
    --conf spark.rapids.sql.format.parquet.multiThreadedRead.enabled=true \
    --conf spark.rapids.sql.format.parquet.multiThreadedRead.numThreads=$((${NUM_EXECUTOR_CORES}*2)) \
    --conf spark.rapids.sql.incompatibleOps.enabled=true \
    --conf spark.rapids.sql.variableFloatAgg.enabled=true \
    --conf spark.rapids.sql.hasNans=true \
    --conf spark.rapids.sql.castFloatToString.enabled=true \
    --conf spark.rapids.sql.castStringToInteger.enabled=true \
    --conf spark.rapids.sql.castStringToFloat.enabled=true \
    --conf spark.storage.blockManagerSlaveTimeoutMs=3600s \
    --conf spark.locality.wait=0s \
    --conf spark.network.timeout=2000s \
    --conf spark.executor.extraJavaOptions=-Dai.rapids.cudf.prefer-pinned=true\ -Dai.rapids.spark.semaphore.enabled=true\ -Dai.rapids.spark.memory.gpu.rmm.init.task=false \
    --jars $SPARK_RAPIDS_PLUGIN_JAR,$SPARK_CUDF_JAR \
    --class com.nvidia.spark.rapids.tests.BenchmarkRunner \
    $SPARK_RAPIDS_PLUGIN_INTEGRATION_TEST_JAR \
    --benchmark tpcds \
    --input ${INPUT_PATH} \
    --input-format parquet \
    --output ${OUTPUT_PATH} \
    --output-format parquet \
    --summary-file-prefix tpcds-${SLURM_JOB_ID} \
    --iterations ${ITERATIONS} \
    --append-dat \
    --query ${i}; done'
else    	
## Keep cluster alive for additional testing or review
   sleep infinity
fi

## Fix ID's to collect results
srun --pack-group=0 -n 1 -N 1 -w `hostname` docker exec -i master /bin/sh -c 'cd ${MOUNT}; chmod -R 755 *; chown -R ${IDU}:${IDG} ${CONFIG_MOUNT}/job/${SLURM_JOB_ID}/*'

## Cleanup (only works if sleep is disabled)
echo "testing complete, please check tpcds-benchmark-results-$SLURM_JOB_ID for relevent output data." 
srun --pack-group=0 -n 1 -N 1 -w `hostname` bash -c 'if [ $(docker ps |grep -c master) -eq 1 ] ; then docker kill master && echo "killing master"; fi'
srun --pack-group=1 --ntasks="${SLURM_JOB_NUM_NODES_PACK_GROUP_1}" bash -c 'if [ $(docker ps |grep -c worker) -eq 1 ] ; then docker kill worker && echo "killing worker"; fi'
##srun --pack-group=0 -n 1 -N 1 --gpus=0 -w `hostname` mkdir tpcds-benchmark-results-${SLURM_JOB_ID}
srun --pack-group=0 -n 1 -N 1 --gpus=0 -w `hostname` mv ${CONFIG_MOUNT}/job/${SLURM_JOB_ID} tpcds-benchmark-results-${SLURM_JOB_ID}
srun --pack-group=0 -n 1 -N 1 --gpus=0 -w `hostname` mv outfile-${SLURM_JOB_ID} tpcds-benchmark-results-${SLURM_JOB_ID}/
##srun --pack-group=0 -n 1 -N 1 --gpus=0 -w `hostname` rm -rf ${CONFIG_MOUNT}/${SLURM_JOB_ID}

echo "Collection & Cleanup complete, bye bye.."
